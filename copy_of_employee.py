# -*- coding: utf-8 -*-
"""Copy of EMPLOYEE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16g8QseRp9sSOfFyeHoYs-6af8rmP8-RU
"""

import streamlit as st

st.title("Employee Income Classification App")

st.write("Preview of the dataset:")
st.dataframe(data.head())

import matplotlib.pyplot as plt

import pandas as pd

# Load the uploaded CSV file
data = pd.read_csv("adult.csv")

# Show first 5 rows
data.head()
print(data.occupation.value_counts())
data.tail(3)

# Check for null values
data.isna().sum()

print(data['workclass'].value_counts())
print(data.dtypes)

data.workclass.replace({'?': 'Others'}, inplace=True)
print(data['workclass'].value_counts())

# Replace '?' with 'Others' in occupation
print(data['occupation'].value_counts())
data.occupation.replace({'?': 'Others'}, inplace=True)
print(data['occupation'].value_counts())

# Remove rows with invalid workclass
data = data[data['workclass'] != 'Without-pay']
data = data[data['workclass'] != 'Never-worked']
print(data['workclass'].value_counts())

# Relationship and gender value counts
print(data.relationship.value_counts())
print(data.gender.value_counts())

data.shape

print(data.nunique())





print(data['age'])

data.replace('?', pd.NA, inplace=True)

data.dropna(inplace=True)

# Filter rows where age > 50
print(data[data['age'] > 50])

print(data[data['income'] == '>50K'])

# Select multiple columns
print(data[['age', 'education', 'income']])

# Frequency count of a column
print(data['occupation'].value_counts())

# Grouping and aggregation
print(data.groupby('education')['income'].value_counts())

# Crosstab (good for classification)
print(pd.crosstab(data['education'], data['income']))

# Save cleaned data
data.to_csv('cleaned_adult.csv', index=False)

Q1 = data['age'].quantile(0.25)
Q3 = data['age'].quantile(0.75)
IQR = Q3 - Q1

# Define lower and upper bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Outlier values
outliers = data[(data['age'] < lower_bound) | (data['age'] > upper_bound)]
print(outliers['age'])

# outlier
# More boxplots
plt.boxplot(data['capital-gain'])
plt.show()

data = data[(data['age'] <= 75) & (data['age'] >= 17)]

plt.boxplot(data['age'])
plt.show()

from sklearn.preprocessing import LabelEncoder

# Initialize the encoder
encoder = LabelEncoder()

# Apply label encoding to categorical columns
data['workclass'] = encoder.fit_transform(data['workclass'])
data['marital-status'] = encoder.fit_transform(data['marital-status'])
data['occupation'] = encoder.fit_transform(data['occupation'])
data['relationship'] = encoder.fit_transform(data['relationship'])
data['race'] = encoder.fit_transform(data['race'])
data['gender'] = encoder.fit_transform(data['gender'])
data['native-country'] = encoder.fit_transform(data['native-country'])
data['income'] = encoder.fit_transform(data['income'])  # Target variable encoding
print(data.head())
print(data['workclass'].unique())
print(data['income'].unique())  # e.g., [0, 1]
print(data.dtypes)

x = data.drop('income', axis=1)  # Features (excluding target)

from sklearn.preprocessing import LabelEncoder

# Encode all object-type (categorical) columns
label_encoder = LabelEncoder()
for col in data.select_dtypes(include='object').columns:
    data[col] = label_encoder.fit_transform(data[col])

from sklearn.preprocessing import LabelEncoder, MinMaxScaler

# Encode categorical columns
label_encoder = LabelEncoder()
for col in data.select_dtypes(include='object').columns:
    data[col] = label_encoder.fit_transform(data[col])

# Separate features and target
x = data.drop('income', axis=1)
y = data['income']

# Scale the features
scaler = MinMaxScaler()
x = scaler.fit_transform(x)
print(x)

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
xtrain, xtest, ytrain, ytest = train_test_split(
    x, y,
    test_size=0.2,
    random_state=23,
    stratify=y
)

# Display the first few rows of training features
print(xtrain[:5])

# Machine Learning algorithm - KNN
from sklearn.neighbors import KNeighborsClassifier

# Initialize the KNN classifier
knn = KNeighborsClassifier()

# Fit the classifier on training data
knn.fit(xtrain, ytrain)

# Predict on the test data
predict = knn.predict(xtest)

# Display the predictions
print(predict)

from sklearn.metrics import accuracy_score

# Calculate and print accuracy
accuracy = accuracy_score(ytest, predict)
print("Accuracy:", accuracy)

from sklearn.neural_network import MLPClassifier
clf = MLPClassifier(solver='adam', hidden_layer_sizes=(5,2), random_state=2, max_iter=2000)
clf.fit(xtrain, ytrain)
predict2 = clf.predict(xtest)
# Display the predictions
print(predict2)

# [51]:
from sklearn.metrics import accuracy_score
accuracy_score(ytest, predict2)

# [51]:
# 0.8391432171356575

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import numpy as np # Import numpy for creating dummy data

# --- Start: Dummy Data for Demonstration ---
# IMPORTANT: Replace this with your actual X (features) and y (target) data.
# This dummy data is just to make the code runnable for testing.
X = np.random.rand(100, 10) # 100 samples, 10 features
y = np.random.randint(0, 2, 100) # 100 samples, 2 classes (0 or 1)
# --- End: Dummy Data for Demonstration ---


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    "LogisticRegression": LogisticRegression(),
    "RandomForest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "GradientBoosting": GradientBoostingClassifier()
}

results = {}

for name, model in models.items():
    pipe = Pipeline([
        ('scaler', StandardScaler()), # Step 1: Scale the data
        ('model', model)             # Step 2: Apply the model
    ])

    # Fit the pipeline on the training data.
    # This step trains the scaler and the model within the pipeline.
    pipe.fit(X_train, y_train)

    # Make predictions on the test data using the fitted pipeline.
    y_pred = pipe.predict(X_test)

    # Calculate accuracy and store it.
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc

    # Print the accuracy and classification report for the current model.
    print(f"--- {name} Results ---")
    print(f"Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred))
    print("\n") # Add a newline for better readability between model results

# You can optionally print all results after the loop
print("\n--- Summary of All Model Accuracies ---")
for name, accuracy in results.items():
    print(f"{name}: {accuracy:.4f}")

import matplotlib.pyplot as plt

plt.bar(results.keys(), results.values(), color='skyblue')
plt.ylabel('Accuracy Score')
plt.title('Model Comparison')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()
